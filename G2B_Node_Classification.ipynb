{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AsuanzQ75Jp",
        "outputId": "0cb67c37-d04b-4469-ddcf-81172cc2fe23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brain\n",
            "/content/drive/MyDrive/InterpretableClustering-master/Brain.npz\n",
            "torch.Size([5000, 12, 20]) torch.Size([5000, 12, 5000]) torch.Size([5000]) torch.Size([3500]) torch.Size([1000]) torch.Size([500])\n",
            "10\n",
            "\n",
            "MyGNN\n",
            "\n",
            "\n",
            "Iteration: 0\n",
            "\n",
            "tensor(2.3183, grad_fn=<NllLossBackward0>) 0.086\n",
            "Epoch: 0001 loss_train: 2.3251 acc_train: 0.0909 loss_val: 2.3183 acc_val: 0.0860 time: 53.7982s\n",
            "tensor(2.3143, grad_fn=<NllLossBackward0>) 0.091\n",
            "Epoch: 0002 loss_train: 2.3194 acc_train: 0.0960 loss_val: 2.3143 acc_val: 0.0910 time: 57.1359s\n",
            "tensor(2.3102, grad_fn=<NllLossBackward0>) 0.098\n",
            "Epoch: 0003 loss_train: 2.3144 acc_train: 0.0997 loss_val: 2.3102 acc_val: 0.0980 time: 54.1844s\n",
            "tensor(2.3060, grad_fn=<NllLossBackward0>) 0.097\n",
            "Epoch: 0004 loss_train: 2.3095 acc_train: 0.0977 loss_val: 2.3060 acc_val: 0.0970 time: 54.3532s\n",
            "tensor(2.3018, grad_fn=<NllLossBackward0>) 0.099\n",
            "Epoch: 0005 loss_train: 2.3046 acc_train: 0.1077 loss_val: 2.3018 acc_val: 0.0990 time: 55.0856s\n",
            "tensor(2.2975, grad_fn=<NllLossBackward0>) 0.104\n",
            "Epoch: 0006 loss_train: 2.2995 acc_train: 0.1103 loss_val: 2.2975 acc_val: 0.1040 time: 53.0684s\n",
            "tensor(2.2930, grad_fn=<NllLossBackward0>) 0.106\n",
            "Epoch: 0007 loss_train: 2.2946 acc_train: 0.1091 loss_val: 2.2930 acc_val: 0.1060 time: 53.2047s\n",
            "tensor(2.2884, grad_fn=<NllLossBackward0>) 0.108\n",
            "Epoch: 0008 loss_train: 2.2908 acc_train: 0.1129 loss_val: 2.2884 acc_val: 0.1080 time: 53.2871s\n",
            "tensor(2.2836, grad_fn=<NllLossBackward0>) 0.113\n",
            "Epoch: 0009 loss_train: 2.2857 acc_train: 0.1243 loss_val: 2.2836 acc_val: 0.1130 time: 53.1721s\n",
            "tensor(2.2788, grad_fn=<NllLossBackward0>) 0.115\n",
            "Epoch: 0010 loss_train: 2.2811 acc_train: 0.1303 loss_val: 2.2788 acc_val: 0.1150 time: 53.5444s\n",
            "tensor(2.2738, grad_fn=<NllLossBackward0>) 0.132\n",
            "Epoch: 0011 loss_train: 2.2749 acc_train: 0.1469 loss_val: 2.2738 acc_val: 0.1320 time: 52.8508s\n",
            "tensor(2.2687, grad_fn=<NllLossBackward0>) 0.13\n",
            "Epoch: 0012 loss_train: 2.2689 acc_train: 0.1546 loss_val: 2.2687 acc_val: 0.1300 time: 53.2393s\n",
            "tensor(2.2635, grad_fn=<NllLossBackward0>) 0.138\n",
            "Epoch: 0013 loss_train: 2.2628 acc_train: 0.1489 loss_val: 2.2635 acc_val: 0.1380 time: 52.9395s\n",
            "tensor(2.2583, grad_fn=<NllLossBackward0>) 0.129\n",
            "Epoch: 0014 loss_train: 2.2589 acc_train: 0.1431 loss_val: 2.2583 acc_val: 0.1290 time: 52.9864s\n",
            "tensor(2.2531, grad_fn=<NllLossBackward0>) 0.126\n",
            "Epoch: 0015 loss_train: 2.2539 acc_train: 0.1314 loss_val: 2.2531 acc_val: 0.1260 time: 53.3353s\n",
            "tensor(2.2479, grad_fn=<NllLossBackward0>) 0.135\n",
            "Epoch: 0016 loss_train: 2.2480 acc_train: 0.1251 loss_val: 2.2479 acc_val: 0.1350 time: 53.0182s\n",
            "tensor(2.2427, grad_fn=<NllLossBackward0>) 0.119\n",
            "Epoch: 0017 loss_train: 2.2422 acc_train: 0.1209 loss_val: 2.2427 acc_val: 0.1190 time: 53.0145s\n",
            "tensor(2.2375, grad_fn=<NllLossBackward0>) 0.122\n",
            "Epoch: 0018 loss_train: 2.2381 acc_train: 0.1180 loss_val: 2.2375 acc_val: 0.1220 time: 53.2101s\n",
            "tensor(2.2325, grad_fn=<NllLossBackward0>) 0.128\n",
            "Epoch: 0019 loss_train: 2.2328 acc_train: 0.1197 loss_val: 2.2325 acc_val: 0.1280 time: 55.3673s\n",
            "tensor(2.2275, grad_fn=<NllLossBackward0>) 0.151\n",
            "Epoch: 0020 loss_train: 2.2290 acc_train: 0.1371 loss_val: 2.2275 acc_val: 0.1510 time: 53.3645s\n",
            "tensor(2.2226, grad_fn=<NllLossBackward0>) 0.144\n",
            "Epoch: 0021 loss_train: 2.2226 acc_train: 0.1411 loss_val: 2.2226 acc_val: 0.1440 time: 53.0562s\n",
            "tensor(2.2178, grad_fn=<NllLossBackward0>) 0.165\n",
            "Epoch: 0022 loss_train: 2.2163 acc_train: 0.1517 loss_val: 2.2178 acc_val: 0.1650 time: 52.1213s\n",
            "tensor(2.2132, grad_fn=<NllLossBackward0>) 0.161\n",
            "Epoch: 0023 loss_train: 2.2121 acc_train: 0.1569 loss_val: 2.2132 acc_val: 0.1610 time: 51.9917s\n",
            "tensor(2.2086, grad_fn=<NllLossBackward0>) 0.163\n",
            "Epoch: 0024 loss_train: 2.2091 acc_train: 0.1563 loss_val: 2.2086 acc_val: 0.1630 time: 51.8574s\n",
            "tensor(2.2040, grad_fn=<NllLossBackward0>) 0.162\n",
            "Epoch: 0025 loss_train: 2.2042 acc_train: 0.1560 loss_val: 2.2040 acc_val: 0.1620 time: 52.0241s\n",
            "tensor(2.1994, grad_fn=<NllLossBackward0>) 0.163\n",
            "Epoch: 0026 loss_train: 2.1995 acc_train: 0.1557 loss_val: 2.1994 acc_val: 0.1630 time: 51.8829s\n",
            "tensor(2.1950, grad_fn=<NllLossBackward0>) 0.164\n",
            "Epoch: 0027 loss_train: 2.1933 acc_train: 0.1551 loss_val: 2.1950 acc_val: 0.1640 time: 52.1301s\n",
            "tensor(2.1905, grad_fn=<NllLossBackward0>) 0.162\n",
            "Epoch: 0028 loss_train: 2.1906 acc_train: 0.1569 loss_val: 2.1905 acc_val: 0.1620 time: 52.1340s\n",
            "tensor(2.1860, grad_fn=<NllLossBackward0>) 0.162\n",
            "Epoch: 0029 loss_train: 2.1843 acc_train: 0.1569 loss_val: 2.1860 acc_val: 0.1620 time: 51.7162s\n",
            "tensor(2.1813, grad_fn=<NllLossBackward0>) 0.164\n",
            "Epoch: 0030 loss_train: 2.1793 acc_train: 0.1566 loss_val: 2.1813 acc_val: 0.1640 time: 51.8454s\n",
            "1942.8588328361511\n",
            "acc= 0.124, auc= 0.6138313555406425, f1= 0.027359430604982204\n",
            "\n",
            "[(0.124, 0.6138313555406425, 0.027359430604982204)]\n",
            "1\n",
            "0.124,0.6138313555406425,0.027359430604982204\n",
            "1959.1068940162659\n"
          ]
        }
      ],
      "source": [
        "# !pip install dgl\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "def Convert(tup, di):\n",
        "    for a, b in tup:\n",
        "        di.setdefault(a, []).append(b)\n",
        "    return di\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def accuracy(output, labels):\n",
        "#     preds = output.max(1)[1].type_as(labels)\n",
        "#     correct = preds.eq(labels).double()\n",
        "#     correct = correct.sum()\n",
        "#     return correct / len(labels)\n",
        "\n",
        "\n",
        "# def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "#     \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "#     sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "#     indices = torch.from_numpy(\n",
        "#         np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "#     values = torch.from_numpy(sparse_mx.data)\n",
        "#     shape = torch.Size(sparse_mx.shape)\n",
        "#     return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# import torch\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "# from layers import GraphConvolution\n",
        "import dgl\n",
        "from dgl.nn.pytorch.conv import GraphConv, GATConv, SAGEConv, DenseGraphConv\n",
        "\n",
        "\n",
        "def reset_parameters(self):\n",
        "    stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "    self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "class RNNGCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(RNNGCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        self.Lambda.data.uniform_(0.2, 0.2)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # out=[]\n",
        "        now_adj = adj[:, 0, :].clone()\n",
        "        for i in range(1, adj.shape[1]):  # time_steps\n",
        "            now_adj = (1 - self.Lambda) * now_adj + self.Lambda * adj[:, i, :]  # weight decay\n",
        "        one_out = self.gc1(x[:, -1, :], now_adj)\n",
        "        one_out = F.relu(one_out)\n",
        "\n",
        "        one_out = F.dropout(one_out, self.dropout, training=self.training)\n",
        "        one_out = self.gc2(one_out, now_adj)\n",
        "\n",
        "        return F.log_softmax(one_out, dim=1)\n",
        "\n",
        "\n",
        "class TRNNGCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, nnode, use_cuda=False):\n",
        "        super(TRNNGCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "        self.Lambda = Parameter(torch.FloatTensor(nclass, nclass))\n",
        "        self.Lambda.data.uniform_(0.5, 0.5)\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        y = torch.randint(0, nclass, (nnode, 1)).flatten()\n",
        "\n",
        "        if self.use_cuda:\n",
        "            self.H = torch.zeros(nnode, nclass).cuda()\n",
        "        else:\n",
        "            self.H = torch.zeros(nnode, nclass)\n",
        "        self.H[range(self.H.shape[0]), y] = 1\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "\n",
        "        w = self.Lambda.data\n",
        "        w = w.clamp(0, 1)\n",
        "        self.Lambda.data = w\n",
        "        if self.use_cuda:\n",
        "            decay_adj = torch.mm(torch.mm(self.H, self.Lambda), self.H.T).cuda()\n",
        "        else:\n",
        "            decay_adj = torch.mm(torch.mm(self.H, self.Lambda), self.H.T)\n",
        "\n",
        "        now_adj = adj[:, 0, :].clone()  # torch.zeros(adj.shape[0], adj.shape[2])\n",
        "        for i in range(1, adj.shape[1]):  # time_steps\n",
        "            now_adj = (1 - decay_adj) * now_adj + decay_adj * adj[:, i, :]\n",
        "        del decay_adj\n",
        "        one_out = F.relu(self.gc1(x[:, -1, :], now_adj))\n",
        "\n",
        "        one_out = F.dropout(one_out, self.dropout, training=self.training)\n",
        "        one_out = self.gc2(one_out, now_adj)\n",
        "        output = F.log_softmax(one_out, dim=1)\n",
        "        y = torch.argmax(output, dim=1)\n",
        "        H_shape = self.H.shape\n",
        "        del self.H\n",
        "        del now_adj\n",
        "        if self.use_cuda:\n",
        "            self.H = torch.zeros(H_shape).cuda()\n",
        "        else:\n",
        "            self.H = torch.zeros(H_shape)\n",
        "        self.H[range(H_shape[0]), y] = 1\n",
        "        return output\n",
        "\n",
        "\n",
        "class LSTMGCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(LSTMGCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "        self.LS_begin = nn.LSTM(input_size=nfeat, hidden_size=nhid, num_layers=1, dropout=0.5, batch_first=True)\n",
        "\n",
        "        self.nhid = nhid\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        adj = self.LS_begin(adj)\n",
        "        x = F.relu(self.gc1(x[:, -1, :], adj[0][:, -1, :]))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj[0][:, -1, :])\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "# class MyGNN(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "#         super(MyGNN, self).__init__()\n",
        "\n",
        "#         self.nhid = nhid\n",
        "#         self.nclass = nclass\n",
        "#         self.num_layers = 2\n",
        "#         self.dropout = dropout\n",
        "\n",
        "#         # self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "#         # print(self.gc1)\n",
        "#         # self.gc2 = GraphConvolution(nhid, nhid)\n",
        "#         # print(self.gc2)\n",
        "#         # self.gc1=SAGEConv()\n",
        "\n",
        "#         self.gc1 = SAGEConv(nfeat, nhid, aggregator_type='mean')\n",
        "#         self.gc2 = SAGEConv(nhid, nhid, aggregator_type='mean')\n",
        "#         # self.gc3 = GATConv(nhid, nhid, num_heads=1)\n",
        "#         # self.gc1 = GraphConv(nfeat, nhid)\n",
        "#         # self.gc2 = GraphConv(nhid, nhid)\n",
        "#         # print(self.dropout)\n",
        "\n",
        "#         self.LS_end = nn.LSTM(input_size=nhid, hidden_size=nclass, num_layers=2, batch_first=True, bidirectional=True)\n",
        "\n",
        "#         # print(self.LS_end)\n",
        "#         # print(\"\\n.................\\n\")\n",
        "\n",
        "#         # self.linear22 = nn.Linear(nclass, nclass) # actual\n",
        "#         self.linear22 = nn.Linear(nhid * 2, nclass)\n",
        "#         self.gc3 = GATConv(nclass, nclass, num_heads=1)\n",
        "\n",
        "#         # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "#         # self.Lambda.data.uniform_(0.2, 0.2)\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "#         # print(x.shape,adj.shape,sep=\"-------------\")\n",
        "\n",
        "#         out = []\n",
        "#         # print(adj)\n",
        "#         # count=0\n",
        "\n",
        "#         # now_adj = adj[:, 0, :].clone()\n",
        "\n",
        "#         # time_wise_attention = []\n",
        "#         # all_time_rank = []\n",
        "#         full_time_edges=set()\n",
        "#         for i in range(0, adj.shape[1]):\n",
        "\n",
        "#             # lol = adj[ :,i, :]\n",
        "#             # print(lol,lol.shape)\n",
        "#             # print(lol[0][1],lol[1][0])\n",
        "#             # my_x = lol.cpu().detach().numpy()\n",
        "#             # my_x=lol.reshape(lol.shape[0],lol.shape[1])\n",
        "#             # print(my_x)\n",
        "#             # print(my_x[0][1],my_x[1][0])\n",
        "\n",
        "#             # b = my_x.transpose()\n",
        "#             # if np.allclose(my_x, b, rtol=0, atol=0):\n",
        "#             #     print(\"The array is Symmetric\")\n",
        "#             # else:\n",
        "#             #     print(\"The array is NOT Symmetric\")\n",
        "#             # break\n",
        "\n",
        "#             # kkk = np.diagonal(my_x).copy()\n",
        "#             # print(kkk, kkk.shape)\n",
        "#             # print(len(set(kkk)))\n",
        "\n",
        "\n",
        "#             # device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "#             # count +=1\n",
        "#             # print(adj[:, i, :],adj[:, i, :].shape,sep=\"&&&&&&&&&&&&&&&&\")\n",
        "\n",
        "#             # now_adj = (1 - self.Lambda) * now_adj + self.Lambda * adj[:, i, :]  # weight decay\n",
        "#             # adj2 = dgl.from_networkx(nx.Graph(now_adj.cpu().detach().numpy()))\n",
        "#             # adj2 = dgl.add_self_loop(adj2)\n",
        "\n",
        "#             # print(i,type(adj))\n",
        "\n",
        "\n",
        "#             adj2 = dgl.from_networkx(nx.Graph(adj[:, i, :].numpy()))\n",
        "#             # # print(i,type(adj2))\n",
        "\n",
        "\n",
        "#             adj2 = dgl.add_self_loop(adj2)\n",
        "\n",
        "#             # print(\"Number of Edges = \",str(len(adj2.edges()[0])))\n",
        "#             # p=adj2.edges()\n",
        "#             # q = list(zip(p[0], p[1]))\n",
        "#             # q= sorted([(int(x),int(y)) for (x,y) in q])\n",
        "#             # print(q)\n",
        "\n",
        "#             # print(\"\\nTime: \",str(i),\", Nodes: \",str(adj2.number_of_nodes()),\", Edges: \",str(adj2.number_of_edges()),\"\\n\")\n",
        "\n",
        "#             a_list = list(zip([int(x) for x in adj2.edges()[0]], [int(x) for x in adj2.edges()[1]]))\n",
        "\n",
        "#             h0 = torch.zeros(self.num_layers * 2, x[:, i, :].size(0), self.nhid)  # 2 for bidirection\n",
        "#             c0 = torch.zeros(self.num_layers * 2, x[:, i, :].size(0), self.nhid)\n",
        "\n",
        "#             one_out = F.relu(self.gc1(adj2, x[:, i, :]))\n",
        "#             # print(one_out.shape)\n",
        "#             one_out = F.dropout(one_out, self.dropout, training=self.training)\n",
        "#             # print(one_out.shape)\n",
        "#             one_out = self.gc2(adj2, one_out)\n",
        "#             one_out = F.dropout(one_out, self.dropout, training=self.training)\n",
        "#             # one_out, yy = self.gc3(adj2, one_out, get_attention=True)\n",
        "#             one_out = self.gc3(adj2, one_out, get_attention=False)\n",
        "\n",
        "#             # weight_dict = list(zip(a_list, [float(x) for x in yy]))\n",
        "#             # # print(weight_dict)\n",
        "#             # # print(yy,yy.shape)\n",
        "\n",
        "#             # # length = len(weight_dict)\n",
        "#             # # middle_index = length//2\n",
        "#             # # first_half = weight_dict[:middle_index]\n",
        "\n",
        "#             # # print(weight_dict)\n",
        "#             # # print(first_half)\n",
        "#             # new_dict = {}\n",
        "#             # for i2, e2 in enumerate(weight_dict):\n",
        "#             #     # print(i,e)\n",
        "#             #     if(e2[0][0]==e2[0][1]):\n",
        "#             #         continue\n",
        "#             #     new_dict[e2[0]] = e2[1]\n",
        "\n",
        "\n",
        "#             # # print(\"Number of Dict = \",str(len(new_dict)))\n",
        "#             # # print(new_dict)\n",
        "#             # r=sorted(list(new_dict.keys()))\n",
        "#             # # print(len(r))\n",
        "#             # ww=list()\n",
        "#             # another_dict={}\n",
        "#             # for x2 in r:\n",
        "#             #     if((x2[0],x2[1]) in r):\n",
        "#             #         # continue\n",
        "#             #         if(x2[0]<x2[1]):\n",
        "#             #             avg_new=(new_dict[(x2[1],x2[0])]+new_dict[(x2[0],x2[1])])/2\n",
        "#             #             another_dict[(x2[0],x2[1])]=avg_new\n",
        "#             #             full_time_edges.add((x2[0],x2[1]))\n",
        "#             #         # del new_dict[(x2[1],x2[0])]\n",
        "#             #     # ww.append(x2)\n",
        "\n",
        "#             # # print(len(another_dict),another_dict)\n",
        "#             # haha={k: v for k, v in sorted(another_dict.items(), key=lambda item: item[1],reverse=True)}\n",
        "#             # # print(haha)\n",
        "#             # edge_rank=zip(list(haha),range(len(haha)))\n",
        "\n",
        "\n",
        "#             # haha_dictionary = {}\n",
        "#             # all_time_rank.append(Convert(edge_rank, haha_dictionary))\n",
        "\n",
        "#             # # print(list(set(q) - set(r)))\n",
        "\n",
        "\n",
        "\n",
        "#             # time_wise_attention.append(new_dict)\n",
        "#             # yy = yy.reshape(1, -1)\n",
        "#             # yy = yy.squeeze()\n",
        "#             # print(yy,yy.shape)\n",
        "#             # print(one_out.shape)\n",
        "#             one_out = one_out.reshape(one_out.shape[0], one_out.shape[2])\n",
        "#             # print(one_out.shape)\n",
        "#             # xx,yy= self.conv4(adj, x, get_attention=True)\n",
        "#             # yy = yy.reshape(1, -1)\n",
        "#             # yy = yy.squeeze()\n",
        "#             # print(yy,yy.shape)\n",
        "#             # print(\"\\n***************************\\n\")\n",
        "#             # one_out = self.linear()\n",
        "#             # print(str(count)+\" \"+\"Inside Loop: \"+str(one_out.shape)+\"\\n\")\n",
        "#             # break\n",
        "#             out += [one_out]\n",
        "#             # print(\"\\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\\n\")\n",
        "#         # print(type(out))\n",
        "#         # print(len(out))\n",
        "#         # print(out[0].shape)\n",
        "#         # print(count)\n",
        "#         # yy_dict={}\n",
        "#         # for i5,e5 in enumerate(list(full_time_edges)):\n",
        "#         #\n",
        "#         #     for i4,e4 in enumerate(all_time_rank):\n",
        "#         #         if(e5 in e4):\n",
        "#         #             yy_dict[e5] += str(e4[e5][0])\n",
        "#         #             # print(e5,e4[e5][0])\n",
        "#         #         else:\n",
        "#         #             yy_dict[e5] = \"\"\n",
        "#         # print(yy_dict)\n",
        "\n",
        "#         # print(full_time_edges)\n",
        "#         # all_time_rank=all_time_rank.reverse()\n",
        "#         # for i4,e4 in enumerate(all_time_rank):\n",
        "\n",
        "#         #     print(i4,e4)\n",
        "#         out = torch.stack(out, 1)\n",
        "#         # print(type(out))\n",
        "\n",
        "#         # print(len(time_wise_attention))\n",
        "\n",
        "#         # print(self.LS_end(out)[0][:, -1, :]) #taking the last hidden state\n",
        "\n",
        "#         # print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
        "#         # out, _ = self.lstm(x, (h0, c0))\n",
        "#         out = self.LS_end(out, (h0, c0))[0][:, -1, :]\n",
        "\n",
        "#         out = self.linear22(out)\n",
        "#         # print(out.shape)\n",
        "#         # out,yy = self.gc3(adj2,out,get_attention=True)\n",
        "#         # yy = yy.reshape(1, -1)\n",
        "#         # yy = yy.squeeze()\n",
        "#         # print(yy,yy.shape)\n",
        "#         # print(out.shape)\n",
        "#         # out = out.reshape(out.shape[0], out.shape[2])\n",
        "\n",
        "#         # print(time_wise_attention)\n",
        "\n",
        "#         # print(\"\\n-------------------\\n\")\n",
        "#         return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "class MyGNN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(MyGNN, self).__init__()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nclass = nclass\n",
        "        self.num_layers=2\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        # print(self.gc1)\n",
        "        # self.gc2 = GraphConvolution(nhid, nhid)\n",
        "        # print(self.gc2)\n",
        "        # self.gc1=SAGEConv()\n",
        "\n",
        "        self.gc1 = SAGEConv(nfeat, nhid, aggregator_type='mean')\n",
        "        self.gc2 = SAGEConv(nhid, nhid, aggregator_type='mean')\n",
        "        # self.gc3 = GATConv(nhid, nhid, num_heads=1)\n",
        "        # self.gc1 = GraphConv(nfeat, nhid)\n",
        "        # self.gc2 = GraphConv(nhid, nhid)\n",
        "        # print(self.dropout)\n",
        "\n",
        "        self.LS_end = nn.LSTM(input_size=nhid, hidden_size=nclass, num_layers=2, batch_first=True,bidirectional=True)\n",
        "        \n",
        "        # print(self.LS_end)\n",
        "        # print(\"\\n.................\\n\")\n",
        "\n",
        "        # self.linear22 = nn.Linear(nclass, nclass) # actual\n",
        "        self.linear22 = nn.Linear(nhid*2, nclass)\n",
        "        # self.gc3 = GATConv(nhid, nclass, num_heads=1)\n",
        "\n",
        "        self.gc3 = GATConv(nhid, nhid, num_heads=1)\n",
        "\n",
        "\n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # self.Lambda.data.uniform_(0.2, 0.2)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # print(x.shape,adj.shape,sep=\"-------------\")\n",
        "\n",
        "\n",
        "        out = []\n",
        "        # print(adj)\n",
        "        # count=0\n",
        "\n",
        "        now_adj = adj[:, 0, :].clone()\n",
        "\n",
        "        time_wise_attention=[]\n",
        "\n",
        "        \n",
        "\n",
        "        for i in range(1,adj.shape[1]):\n",
        "            # device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "            # count +=1\n",
        "            # print(adj[:, i, :],adj[:, i, :].shape,sep=\"&&&&&&&&&&&&&&&&\")\n",
        "\n",
        "\n",
        "\n",
        "            # now_adj = (1 - self.Lambda) * now_adj + self.Lambda * adj[:, i, :]  # weight decay\n",
        "            # adj2 = dgl.from_networkx(nx.Graph(now_adj.cpu().detach().numpy()))\n",
        "            # adj2 = dgl.add_self_loop(adj2)\n",
        "\n",
        "\n",
        "            adj2 = dgl.from_networkx(nx.Graph(adj[:, i, :].numpy()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            adj2 = dgl.add_self_loop(adj2)\n",
        "\n",
        "            \n",
        "            # print(\"\\nTime: \",str(i),\", Nodes: \",str(adj2.number_of_nodes()),\", Edges: \",str(adj2.number_of_edges()),\"\\n\")\n",
        "\n",
        "\n",
        "            # a_list=list(zip([int(x) for x in adj2.edges()[0]],[int(x) for x in adj2.edges()[1]]))\n",
        "\n",
        "\n",
        "            h0 = torch.zeros(self.num_layers*2, x[:, i, :].size(0), self.nhid) # 2 for bidirection \n",
        "            c0 = torch.zeros(self.num_layers*2, x[:, i, :].size(0), self.nhid)\n",
        "\n",
        "\n",
        "\n",
        "            # one_out = F.relu(self.gc3(adj2, x[:, i, :]))  #different from self-defined gcn\n",
        "            # one_out=one_out.reshape(one_out.shape[0],one_out.shape[2])\n",
        "\n",
        "\n",
        "            one_out = F.relu(self.gc1(adj2, x[:, i, :]))\n",
        "            # print(one_out.shape)\n",
        "            one_out = F.dropout(one_out, self.dropout, training=self.training)\n",
        "            # # print(one_out.shape)\n",
        "            one_out = self.gc2(adj2,one_out)\n",
        "            one_out = F.dropout(one_out, self.dropout, training=self.training)\n",
        "\n",
        "\n",
        "            one_out,yy = self.gc3(adj2,one_out,get_attention=True)\n",
        "\n",
        "            # weight_dict=list(zip(a_list,[float(x) for x in yy]))\n",
        "            # print(weight_dict)\n",
        "            # print(yy,yy.shape)\n",
        "\n",
        "            # length = len(weight_dict)\n",
        "            # middle_index = length//2\n",
        "            # first_half = weight_dict[:middle_index]\n",
        "\n",
        "            # print(weight_dict)\n",
        "            # print(first_half)\n",
        "            # new_dict={}\n",
        "            # for i2,e2 in enumerate(weight_dict):\n",
        "             \n",
        "            #   new_dict[e2[0]]=e2[1]\n",
        "        \n",
        "            # time_wise_attention.append(new_dict)\n",
        "            # yy = yy.reshape(1, -1)\n",
        "            # yy = yy.squeeze()\n",
        "            # print(yy,yy.shape)\n",
        "            # print(one_out.shape)\n",
        "            one_out = one_out.reshape(one_out.shape[0], one_out.shape[2])\n",
        "            # print(one_out.shape)\n",
        "            # xx,yy= self.conv4(adj, x, get_attention=True)\n",
        "            # yy = yy.reshape(1, -1)\n",
        "            # yy = yy.squeeze()\n",
        "            # print(yy,yy.shape)\n",
        "            # print(\"\\n***************************\\n\")\n",
        "            # one_out = self.linear()\n",
        "            # print(str(count)+\" \"+\"Inside Loop: \"+str(one_out.shape)+\"\\n\")\n",
        "            # break\n",
        "            out += [one_out]\n",
        "        # print(type(out))\n",
        "        # print(len(out))\n",
        "        # print(out[0].shape)\n",
        "        # print(count)\n",
        "        out = torch.stack(out, 1)\n",
        "        # print(type(out))\n",
        "\n",
        "\n",
        "        # print(self.LS_end(out)[0][:, -1, :]) #taking the last hidden state\n",
        "\n",
        "        # print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
        "                # out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.LS_end(out,(h0, c0))[0][:, -1, :]\n",
        "        \n",
        "        out=self.linear22(out)\n",
        "        # print(out.shape)\n",
        "        # out,yy = self.gc3(adj2,out,get_attention=True)\n",
        "        # yy = yy.reshape(1, -1)\n",
        "        # yy = yy.squeeze()\n",
        "        # print(yy,yy.shape)\n",
        "        # print(out.shape)\n",
        "        # out = out.reshape(out.shape[0], out.shape[2])\n",
        "\n",
        "        # print(time_wise_attention)\n",
        "\n",
        "\n",
        "        # print(\"\\n-------------------\\n\")\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "# class GCN(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "#         super(GCN, self).__init__()\n",
        "\n",
        "#         self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "#         self.gc2 = GraphConvolution(nhid, nclass)\n",
        "#         self.dropout = dropout\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "#         x = F.relu(self.gc1(x, adj))\n",
        "#         x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         x = self.gc2(x, adj)\n",
        "\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# class GAT(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "#         super(GAT, self).__init__()\n",
        "#         self.dropout = dropout\n",
        "#         self.conv1 = GATConv(nfeat, nhid, num_heads=1)\n",
        "#         self.conv2 = GATConv(nhid, nclass, num_heads=1)\n",
        "\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "#         # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
        "#         # is the same as the out_degree.\n",
        "#         # Perform graph convolution and activation function.\n",
        "\n",
        "#         x = F.relu(self.conv1(adj, x))  #different from self-defined gcn\n",
        "#         x=x.reshape(x.shape[0],x.shape[2])\n",
        "#         x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         x = self.conv2(adj, x)\n",
        "#         x=x.reshape(x.shape[0],x.shape[2])\n",
        "\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "\n",
        "# class GAT(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "#         super(GAT, self).__init__()\n",
        "#         self.dropout = dropout\n",
        "#         self.conv1 = GATConv(nfeat, nhid, num_heads=1)\n",
        "#         self.conv2 = GATConv(nhid, nclass, num_heads=1)\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "#         # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
        "#         # is the same as the out_degree.\n",
        "#         # Perform graph convolution and activation function.\n",
        "#         print(\"\\n!!!!!!!!!!!!!!!!!! HELLO !!!!!!!!!!!!!!!!!!!!!\\n\")\n",
        "#         print(x.shape)\n",
        "#         x = F.relu(self.conv1(adj, x))  # different from self-defined gcn\n",
        "#         print(\"\\nAfter Conv1:\\n\")\n",
        "#         print(x.shape)\n",
        "#         print(x.shape[0],x.shape[2])\n",
        "#         x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         print(\"\\nAfter 1st Reshape:\\n\")\n",
        "#         print(x.shape)\n",
        "#         x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         print(\"\\nAfter 1st Dropout\\n\")\n",
        "#         print(x.shape)\n",
        "        \n",
        "#         x = self.conv2(adj, x)\n",
        "#         print(\"\\nAfter Conv2\\n\")\n",
        "#         print(x.shape)\n",
        "#         x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         print(\"\\nAfter 2nd Reshape\\n\")\n",
        "#         print(x.shape)\n",
        "#         my_res=F.log_softmax(x, dim=1)\n",
        "#         print(\"\\nReturning log_softmax\\n\")\n",
        "#         print(my_res.shape)\n",
        "#         print(\"\\n===========END======================\\n\")\n",
        "\n",
        "\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# class GraphSage(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "#         super(GraphSage, self).__init__()\n",
        "#         self.dropout = dropout\n",
        "#         self.conv1 = SAGEConv(nfeat, nhid, aggregator_type='mean')\n",
        "#         self.conv2 = SAGEConv(nhid, nclass, aggregator_type='mean')\n",
        "#         # self.conv3 = GATConv(nclass, 4, num_heads=8)\n",
        "#         # self.conv4 = GATConv(4, 20, num_heads=8)\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "#         # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
        "#         # is the same as the out_degree.\n",
        "#         # Perform graph convolution and activation function.\n",
        "#         x = F.relu(self.conv1(adj, x))  # different from self-defined gcn\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         x = self.conv2(adj, x)\n",
        "#         # x = self.conv3(adj, x)\n",
        "#         # x = self.conv4(adj, x)\n",
        "\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "\n",
        "# class GraphSage_BiLSTM_GAT(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "#         super(GraphSage_BiLSTM_GAT, self).__init__()\n",
        "\n",
        "#         self.dropout = dropout\n",
        "\n",
        "#         # self.conv1 = GATConv(nfeat, nhid, num_heads=1)\n",
        "#         # self.conv2 = GATConv(nhid, nclass, num_heads=1)\n",
        "#         # self.conv3 = GATConv(nclass, nclass, num_heads=1)\n",
        "#         # self.conv4 = GATConv(nclass, nclass, num_heads=1)\n",
        "#         # self.conv5 = GATConv(nclass, nclass, num_heads=1)\n",
        "\n",
        "#         self.conv1 = SAGEConv(nfeat, nhid, aggregator_type='mean')\n",
        "#         self.conv2 = SAGEConv(nhid, nclass, aggregator_type='mean')\n",
        "#         # self.conv3 = SAGEConv(nhid, nhid, aggregator_type='mean')\n",
        "#         # self.conv4 = SAGEConv(nhid, nhid, aggregator_type='mean')\n",
        "#         # self.conv5 = SAGEConv(nhid, nhid, aggregator_type='mean')\n",
        "\n",
        "#         self.conv6 = GATConv(nhid, nclass, num_heads=1)\n",
        "#         # self.conv7 = GATConv(nhid, nhid, num_heads=1)\n",
        "#         # self.conv6 = GATConv(nhid, nhid, num_heads=1)\n",
        "\n",
        "\n",
        "#         # self.conv8 = GATConv(nhid, nhid, num_heads=16)\n",
        "\n",
        "#         self.LS_end = nn.LSTM(input_size=nclass, hidden_size=nclass, num_layers=8, dropout=dropout, batch_first=True,\n",
        "#                               bidirectional=True)\n",
        "\n",
        "\n",
        "#         # self.conv3 = SAGEConv(nclass, nclass, aggregator_type='mean')\n",
        "#         self.conv9 = GATConv(nhid, nclass, num_heads=1)\n",
        "#         # self.conv5 = GATConv(nclass, nclass, num_heads=1)\n",
        "#         # self.conv6 = GATConv(nclass, nclass, num_heads=1)\n",
        "#         # self.conv4 = SAGEConv(nclass, nclass, aggregator_type='mean')\n",
        "\n",
        "#         # self.conv2 = SAGEConv(nhid, nclass, aggregator_type='mean')\n",
        "\n",
        "#         # self.conv3 = SAGEConv(nclass, nclass, aggregator_type='mean')\n",
        "\n",
        "#         # self.conv4 = SAGEConv(nclass, nclass, aggregator_type='mean')\n",
        "\n",
        "#         # self.conv5 = SAGEConv(nclass, nclass, aggregator_type='mean')\n",
        "\n",
        "#         # self.conv3 = GATConv(nclass, nclass, num_heads=16)\n",
        "\n",
        "#         # self.LS_end = nn.LSTM(input_size=nclass, hidden_size=nclass, num_layers=8, dropout=dropout, batch_first=True,\n",
        "#         #                       bidirectional=True)\n",
        "\n",
        "#         # self.conv4 = GATConv(nfeat, nclass, num_heads=16)\n",
        "#         # self.conv2 = GATConv(nhid, nclass, num_heads=1)\n",
        "\n",
        "#         # self.dropout = dropout\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "#         # xx,yy= self.conv4(adj, x, get_attention=True)\n",
        "#         # yy = yy.reshape(1, -1)\n",
        "#         # yy = yy.squeeze()\n",
        "#         # print(yy,yy.shape)\n",
        "\n",
        "#         x = F.relu(self.conv1(adj, x))  # different from self-defined gcn\n",
        "#         x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         x = self.conv2(adj, x)\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # x = self.conv3(adj, x)\n",
        "#         # print(x.shape)\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # x = self.conv4(adj, x)\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # x = self.conv5(adj, x)\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # x = F.relu(self.conv1(adj, x))  # different from self-defined gcn\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # x = self.conv2(adj, x)\n",
        "#         # x = self.conv3(adj, x)\n",
        "#         # x = self.conv4(adj, x)\n",
        "#         # x = self.conv5(adj, x)\n",
        "\n",
        "#         # x = F.relu(self.conv1(adj, x))  # different from self-defined gcn\n",
        "#         # x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # x = self.conv6(adj, x)\n",
        "#         # x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # x = self.conv3(adj, x)\n",
        "#         # x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # # x = self.conv4(adj, x)\n",
        "\n",
        "#         # x = self.conv3(adj, x)\n",
        "#         # # x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         # # x = self.conv4(adj, x)\n",
        "#         # # # x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         # # x = self.conv5(adj, x)\n",
        "#         # # x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         # x = self.conv6(adj, x)\n",
        "#         # x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         # x = self.conv7(adj, x)\n",
        "#         # x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         # x = self.conv8(adj, x)\n",
        "#         # x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         # x = self.conv9(adj, x)\n",
        "#         # x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         # print(x.shape)\n",
        "#         # print(x[0])\n",
        "#         # print(x[1])\n",
        "#         # x = self.LS_end(x)\n",
        "#         # x = x.reshape(x.shape[0], x.shape[2])\n",
        "\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "\n",
        "# class GraphSage_BiLSTM_GAT(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "#         super(GraphSage_BiLSTM_GAT, self).__init__()\n",
        "#         self.dropout = dropout\n",
        "\n",
        "#         self.conv1 = SAGEConv(nfeat, nhid, aggregator_type='mean')\n",
        "#         self.conv2 = SAGEConv(nhid, nhid, aggregator_type='mean')\n",
        "#         self.conv3 = GATConv(nhid, nhid, num_heads=1)\n",
        "#         self.LS_end = nn.LSTM(input_size=nclass, hidden_size=nclass, num_layers=8, dropout=dropout, batch_first=True,\n",
        "#                               bidirectional=True)\n",
        "#         # self.conv4 = GATConv(nclass, nclass, num_heads=16)\n",
        "#         # self.dropout = dropout\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "\n",
        "#         x = F.relu(self.conv1(adj, x))  # different from self-defined gcn\n",
        "#         x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         x = self.conv2(adj, x)\n",
        "#         x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         # x = self.conv3(adj, x)\n",
        "#         # x = x.reshape(x.shape[0], x.shape[2])\n",
        "\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "# egcn\n",
        "\n",
        "\n",
        "# class GGGGG(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "#         super(GGGGG, self).__init__()\n",
        "\n",
        "#         # self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "#         # self.gc2 = GraphConvolution(nhid, nhid)\n",
        "#         self.dropout = dropout\n",
        "\n",
        "\n",
        "#         self.gc1 = SAGEConv(nfeat, nhid, aggregator_type='mean')\n",
        "#         self.gc2 = SAGEConv(nhid, nclass, aggregator_type='mean')\n",
        "\n",
        "\n",
        "#         self.LS_end = nn.LSTM(input_size=nhid, hidden_size=nclass, num_layers=2, dropout=0.5,\n",
        "#                               batch_first=True)\n",
        "#         self.nhid = nhid\n",
        "#         self.nclass = nclass\n",
        "#         self.linear = nn.Linear(nclass, nclass)\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "#         out = []\n",
        "#         for i in range(adj.shape[1]):\n",
        "#             print(adj[:,i,:],adj[:,i,:].shape,sep=\"****\")\n",
        "#             one_out = F.relu(self.gc1(adj[:, i, :], x[:, i, :]))\n",
        "#             one_out = F.dropout(one_out, self.dropout, training=self.training)\n",
        "#             one_out = self.gc2(adj[:, i, :],one_out)\n",
        "#             out += [one_out]\n",
        "#         # print(out)\n",
        "#         # print(len(out))\n",
        "#         out = torch.stack(out, 1)\n",
        "#         print(\"\\n-------------\\n\")\n",
        "#         # print(out)\n",
        "#         # print(len(out))\n",
        "\n",
        "#         out = self.LS_end(out)[0][:, -1, :]\n",
        "\n",
        "#         return F.log_softmax(out, dim=1)\n",
        "\n",
        "class Namespace(object):\n",
        "    '''\n",
        "    helps referencing object in a dictionary as dict.key instead of dict['key']\n",
        "    '''\n",
        "\n",
        "    def __init__(self, adict):\n",
        "        self.__dict__.update(adict)\n",
        "\n",
        "\n",
        "def pad_with_last_val(vect, k):\n",
        "    device = 'cuda' if vect.is_cuda else 'cpu'\n",
        "    pad = torch.ones(k - vect.size(0),\n",
        "                     dtype=torch.long,\n",
        "                     device=device) * vect[-1]\n",
        "    vect = torch.cat([vect, pad])\n",
        "    return vect\n",
        "\n",
        "\n",
        "# class GAT_BiLSTM(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "#         super(GAT_BiLSTM, self).__init__()\n",
        "#         self.dropout = dropout\n",
        "\n",
        "#         # self.LS_end = nn.LSTM(input_size=nhid, hidden_size=nclass, num_layers=2, dropout=0.5,\n",
        "#         #                       batch_first=True, bidirectional=True)\n",
        "#         self.conv1 = GATConv(nfeat, nhid, num_heads=1)\n",
        "#         self.conv2 = GATConv(nhid, nclass, num_heads=1)\n",
        "\n",
        "#     def forward(self, x, adj):\n",
        "#         # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
        "#         # is the same as the out_degree.\n",
        "#         # Perform graph convolution and activation function.\n",
        "\n",
        "#         x = F.relu(self.conv1(adj, x))  # different from self-defined gcn\n",
        "#         print(x.shape)\n",
        "#         x = x.reshape(x.shape[0], x.shape[2])\n",
        "#         print(x.shape)\n",
        "#         print(\"\\n------------Hello-----------\\n\")\n",
        "#         x = F.dropout(x, self.dropout, training=self.training)\n",
        "#         x = self.conv2(adj, x)\n",
        "#         x = x.reshape(x.shape[0], x.shape[2])\n",
        "\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class GCNLSTM(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCNLSTM, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.LS_end=nn.LSTM(input_size=nhid, hidden_size=nclass, num_layers=2, dropout=0.5,\n",
        "                            batch_first=True)\n",
        "        self.nhid=nhid\n",
        "        self.nclass=nclass\n",
        "        self.linear=nn.Linear(nclass, nclass)\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        out=[]\n",
        "        for i in range(adj.shape[1]):\n",
        "            one_out=F.relu(self.gc1(x[:,i,:],adj[:,i,:]))\n",
        "            one_out = F.dropout(one_out, self.dropout, training=self.training)\n",
        "            one_out = self.gc2(one_out, adj[:,i,:])\n",
        "            out+=[one_out]\n",
        "        out = torch.stack(out, 1)   \n",
        "        out=self.LS_end(out)[0][:,-1,:]\n",
        "\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        \n",
        "\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "\n",
        "        \n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "    \n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = GATConv(nfeat, nhid, num_heads=1)\n",
        "        self.conv2 = GATConv(nhid, nclass, num_heads=1)\n",
        "        \n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
        "        # is the same as the out_degree.\n",
        "        # Perform graph convolution and activation function.\n",
        "        \n",
        "        x = F.relu(self.conv1(adj, x))  #different from self-defined gcn\n",
        "        x=x.reshape(x.shape[0],x.shape[2])\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.conv2(adj, x)\n",
        "        x=x.reshape(x.shape[0],x.shape[2])\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    \n",
        "class GraphSage(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GraphSage, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.conv1 = SAGEConv(nfeat, nhid,aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(nhid, nhid,aggregator_type='mean')\n",
        "        # self.conv3 = GATConv(nhid, nclass,num_heads=1)\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # Use node degree as the initial node feature. For undirected graphs, the in-degree\n",
        "        # is the same as the out_degree.\n",
        "        # Perform graph convolution and activation function.\n",
        "        x = F.relu(self.conv1(adj, x))  #different from self-defined gcn\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.conv2(adj, x)\n",
        "        # x = F.dropout(x, self.dropout, training=self.training)\n",
        "        # x = self.conv3(adj, x)\n",
        "        # x=x.reshape(x.shape[0],x.shape[2])\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)\n",
        "    \n",
        "class GCNLSTM(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCNLSTM, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        self.LS_end=nn.LSTM(input_size=nhid, hidden_size=nclass, num_layers=2, dropout=0.5,\n",
        "                            batch_first=True)\n",
        "        self.nhid=nhid\n",
        "        self.nclass=nclass\n",
        "        self.linear=nn.Linear(nclass, nclass)\n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        out=[]\n",
        "        for i in range(adj.shape[1]):\n",
        "            one_out=F.relu(self.gc1(x[:,i,:],adj[:,i,:]))\n",
        "            one_out = F.dropout(one_out, self.dropout, training=self.training)\n",
        "            one_out = self.gc2(one_out, adj[:,i,:])\n",
        "            out+=[one_out]\n",
        "        out = torch.stack(out, 1)   \n",
        "        out=self.LS_end(out)[0][:,-1,:]\n",
        "\n",
        "        return F.log_softmax(out, dim=1)\n",
        " \n",
        "    \n",
        "    \n",
        "#egcn\n",
        "    \n",
        "class Namespace(object):\n",
        "    '''\n",
        "    helps referencing object in a dictionary as dict.key instead of dict['key']\n",
        "    '''\n",
        "    def __init__(self, adict):\n",
        "        self.__dict__.update(adict)\n",
        "        \n",
        "def pad_with_last_val(vect,k):\n",
        "    device = 'cuda' if vect.is_cuda else 'cpu'\n",
        "    pad = torch.ones(k - vect.size(0),\n",
        "                         dtype=torch.long,\n",
        "                         device = device) * vect[-1]\n",
        "    vect = torch.cat([vect,pad])\n",
        "    return vect\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#only use EGCN\n",
        "\n",
        "class EGCN(torch.nn.Module): #egcn_o\n",
        "    def __init__(self, nfeat, nhid, nclass, device='cpu', skipfeats=False):\n",
        "        super().__init__()\n",
        "        GRCU_args = Namespace({})\n",
        "\n",
        "        feats = [nfeat,\n",
        "                 nhid,\n",
        "                 nhid]\n",
        "        self.device = device\n",
        "        self.skipfeats = skipfeats\n",
        "        self.GRCU_layers = []\n",
        "        self._parameters = nn.ParameterList()\n",
        "        self.mlp = torch.nn.Sequential(torch.nn.Linear(in_features = nhid,out_features = nhid),\n",
        "                                       torch.nn.ReLU(),\n",
        "                                       torch.nn.Linear(in_features = nhid,out_features = nclass))\n",
        "        for i in range(1,len(feats)):\n",
        "            GRCU_args = Namespace({'in_feats' : feats[i-1],\n",
        "                                     'out_feats': feats[i],\n",
        "                                     'activation': torch.nn.RReLU()})\n",
        "\n",
        "            grcu_i = GRCU(GRCU_args)\n",
        "            #print (i,'grcu_i', grcu_i)\n",
        "            self.GRCU_layers.append(grcu_i.to(self.device))\n",
        "            self._parameters.extend(list(self.GRCU_layers[-1].parameters()))\n",
        "        \n",
        "    def parameters(self):\n",
        "        return self._parameters\n",
        "\n",
        "    def forward(self,Nodes_list, A_list):#,nodes_mask_list):\n",
        "        node_feats= Nodes_list[-1]\n",
        "        for unit in self.GRCU_layers:\n",
        "            Nodes_list = unit(A_list,Nodes_list)#,nodes_mask_list)\n",
        "\n",
        "        out = Nodes_list[-1]\n",
        "        if self.skipfeats:\n",
        "            out = torch.cat((out,node_feats), dim=1)   # use node_feats.to_dense() if 2hot encoded input \n",
        "       \n",
        "        \n",
        "        return F.log_softmax(self.mlp(out), dim=1)\n",
        "\n",
        "class GRCU(torch.nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        cell_args = Namespace({})\n",
        "        cell_args.rows = args.in_feats\n",
        "        cell_args.cols = args.out_feats\n",
        "\n",
        "        self.evolve_weights = mat_GRU_cell(cell_args)  \n",
        "\n",
        "        self.activation = self.args.activation\n",
        "        self.GCN_init_weights = Parameter(torch.Tensor(self.args.in_feats,self.args.out_feats))\n",
        "        self.reset_param(self.GCN_init_weights)\n",
        "\n",
        "    def reset_param(self,t):\n",
        "        #Initialize based on the number of columns\n",
        "        stdv = 1. / math.sqrt(t.size(1))\n",
        "        t.data.uniform_(-stdv,stdv)\n",
        "\n",
        "    def forward(self,A_list,node_embs_list):#,mask_list):\n",
        "        GCN_weights = self.GCN_init_weights\n",
        "        out_seq = []\n",
        "        for t,Ahat in enumerate(A_list):\n",
        "            node_embs = node_embs_list[t]\n",
        "            #first evolve the weights from the initial and use the new weights with the node_embs\n",
        "            GCN_weights = self.evolve_weights(GCN_weights)#,node_embs,mask_list[t])\n",
        "            node_embs = self.activation(Ahat.matmul(node_embs.matmul(GCN_weights)))\n",
        "\n",
        "            out_seq.append(node_embs)\n",
        "\n",
        "        return out_seq\n",
        "\n",
        "class mat_GRU_cell(torch.nn.Module):\n",
        "    def __init__(self,args):\n",
        "        super().__init__()\n",
        "        self.args = args\n",
        "        self.update = mat_GRU_gate(args.rows,\n",
        "                                   args.cols,\n",
        "                                   torch.nn.Sigmoid())\n",
        "\n",
        "        self.reset = mat_GRU_gate(args.rows,\n",
        "                                   args.cols,\n",
        "                                   torch.nn.Sigmoid())\n",
        "\n",
        "        self.htilda = mat_GRU_gate(args.rows,\n",
        "                                   args.cols,\n",
        "                                   torch.nn.Tanh())\n",
        "        \n",
        "        self.choose_topk = TopK(feats = args.rows,\n",
        "                                k = args.cols)\n",
        "\n",
        "    def forward(self,prev_Q):#,prev_Z,mask):     ###Same as GCNH\n",
        "        # z_topk = self.choose_topk(prev_Z,mask)\n",
        "        z_topk = prev_Q\n",
        "        update = self.update(z_topk,prev_Q)\n",
        "        reset = self.reset(z_topk,prev_Q)\n",
        "\n",
        "        h_cap = reset * prev_Q\n",
        "        h_cap = self.htilda(z_topk, h_cap)\n",
        "\n",
        "        new_Q = (1 - update) * prev_Q + update * h_cap\n",
        "        return new_Q\n",
        "\n",
        "        \n",
        "\n",
        "class mat_GRU_gate(torch.nn.Module):\n",
        "    def __init__(self,rows,cols,activation):\n",
        "        super().__init__()\n",
        "        self.activation = activation\n",
        "        #the k here should be in_feats which is actually the rows\n",
        "        self.W = Parameter(torch.Tensor(rows,rows))\n",
        "        self.reset_param(self.W)\n",
        "\n",
        "        self.U = Parameter(torch.Tensor(rows,rows))\n",
        "        self.reset_param(self.U)\n",
        "\n",
        "        self.bias = Parameter(torch.zeros(rows,cols))\n",
        "\n",
        "    def reset_param(self,t):\n",
        "        #Initialize based on the number of columns\n",
        "        stdv = 1. / math.sqrt(t.size(1))\n",
        "        t.data.uniform_(-stdv,stdv)\n",
        "\n",
        "    def forward(self,x,hidden):\n",
        "        out = self.activation(self.W.matmul(x) + \\\n",
        "                              self.U.matmul(hidden) + \\\n",
        "                              self.bias)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TopK(torch.nn.Module):\n",
        "    def __init__(self,feats,k):\n",
        "        super().__init__()\n",
        "        self.scorer = Parameter(torch.Tensor(feats,1))\n",
        "        self.reset_param(self.scorer)\n",
        "        \n",
        "        self.k = k\n",
        "\n",
        "    def reset_param(self,t):\n",
        "        #Initialize based on the number of rows\n",
        "        stdv = 1. / math.sqrt(t.size(0))\n",
        "        t.data.uniform_(-stdv,stdv)\n",
        "\n",
        "    def forward(self,node_embs,mask):\n",
        "        scores = node_embs.matmul(self.scorer) / self.scorer.norm()\n",
        "        scores = scores + mask\n",
        "\n",
        "        vals, topk_indices = scores.view(-1).topk(self.k)\n",
        "        topk_indices = topk_indices[vals > -float(\"Inf\")]\n",
        "\n",
        "        if topk_indices.size(0) < self.k:\n",
        "            topk_indices = u.pad_with_last_val(topk_indices,self.k)\n",
        "            \n",
        "        tanh = torch.nn.Tanh()\n",
        "\n",
        "        if isinstance(node_embs, torch.sparse.FloatTensor) or \\\n",
        "           isinstance(node_embs, torch.cuda.sparse.FloatTensor):\n",
        "            node_embs = node_embs.to_dense()\n",
        "\n",
        "        out = node_embs[topk_indices] * tanh(scores[topk_indices].view(-1,1))\n",
        "\n",
        "        #we need to transpose the output\n",
        "        return out.t()\n",
        "        \n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "# from utils import encode_onehot\n",
        "# from models import MyGNN,GCN,GAT,GraphSage,EGCN,LSTMGCN,RNNGCN,TRNNGCN\n",
        "\n",
        "# import tensorflow\n",
        "# from dynamicgem.embedding.dynAERNN import DynAERNN\n",
        "\n",
        "import dgl\n",
        "\n",
        "import scipy as sp\n",
        "import scipy.linalg as linalg\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.vq import kmeans, vq\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn import metrics\n",
        "\n",
        "from itertools import permutations\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "\n",
        "def getNormLaplacian(W):\n",
        "    \"\"\"input matrix W=(w_ij)\n",
        "    \"compute D=diag(d1,...dn)\n",
        "    \"and L=D-W\n",
        "    \"and Lbar=D^(-1/2)LD^(-1/2)\n",
        "    \"return Lbar\n",
        "    \"\"\"\n",
        "    d = [np.sum(row) for row in W]\n",
        "    D = np.diag(d)\n",
        "    L = D - W\n",
        "    Dn = np.power(np.linalg.matrix_power(D, -1), 0.5)\n",
        "    Lbar = np.dot(np.dot(Dn, L), Dn)\n",
        "    return Lbar\n",
        "\n",
        "\n",
        "def getKlargestEigVec(Lbar, k):\n",
        "    \"\"\"input\n",
        "    \"matrix Lbar and k\n",
        "    \"return\n",
        "    \"k largest eigen values and their corresponding eigen vectors\n",
        "    \"\"\"\n",
        "    eigval, eigvec = linalg.eig(Lbar)\n",
        "    dim = len(eigval)\n",
        "\n",
        "    # find top k largest eigval\n",
        "    dictEigval = dict(zip(eigval, range(0, dim)))\n",
        "    kEig = np.sort(eigval)[::-1][:k]  # [0:k]\n",
        "    ix = [dictEigval[k] for k in kEig]\n",
        "    return eigval[ix], eigvec[:, ix]\n",
        "\n",
        "\n",
        "def getKlargestSigVec(Lbar, k):\n",
        "    \"\"\"input\n",
        "    \"matrix Lbar and k\n",
        "    \"return\n",
        "    \"k largest singular values and their corresponding eigen vectors\n",
        "    \"\"\"\n",
        "    lsigvec, sigval, rsigvec = linalg.svd(Lbar)\n",
        "    dim = len(sigval)\n",
        "\n",
        "    # find top k largest left sigval\n",
        "    dictSigval = dict(zip(sigval, range(0, dim)))\n",
        "    kSig = np.sort(sigval)[::-1][:k]  # [0:k]\n",
        "    ix = [dictSigval[k] for k in kSig]\n",
        "    return sigval[ix], lsigvec[:, ix]\n",
        "\n",
        "\n",
        "def checkResult(Lbar, eigvec, eigval, k):\n",
        "    \"\"\"\n",
        "    \"input\n",
        "    \"matrix Lbar and k eig values and k eig vectors\n",
        "    \"print norm(Lbar*eigvec[:,i]-lamda[i]*eigvec[:,i])\n",
        "    \"\"\"\n",
        "    check = [np.dot(Lbar, eigvec[:, i]) - eigval[i] * eigvec[:, i] for i in range(0, k)]\n",
        "    length = [np.linalg.norm(e) for e in check] / np.spacing(1)\n",
        "    print(\"Lbar*v-lamda*v are %s*%s\" % (length, np.spacing(1)))\n",
        "\n",
        "\n",
        "\"\"\"# Model\"\"\"\n",
        "\n",
        "\n",
        "def one_hot(l, classnum=1):  # classnum fix some special case\n",
        "    one_hot_l = np.zeros((len(l), max(l.max() + 1, classnum)))\n",
        "    for i in range(len(l)):\n",
        "        one_hot_l[i][l[i]] = 1\n",
        "    return one_hot_l\n",
        "\n",
        "\n",
        "def train(epoch, model, optimizer, features, adj, labels, idx_train, idx_val, model_type,file_name):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    # print(features.shape)\n",
        "    # print(\"\\nNow in Training: \\n\")\n",
        "    output = model(features, adj)\n",
        "    # print(output.shape)\n",
        "\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "\n",
        "    pred_labels = torch.argmax(output, axis=1)\n",
        "    acc_train = metrics.accuracy_score(pred_labels[idx_train].cpu().detach().numpy(),\n",
        "                                       labels[idx_train].cpu().detach().numpy())\n",
        "\n",
        "    # print(acc_train)\n",
        "    # train_acc = (pred_labels[idx_train].cpu().detach().numpy() == labels[idx_train].cpu().detach().numpy()).float().mean()\n",
        "    # print(train_acc)\n",
        "\n",
        "    loss_train.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    # print(loss_train,acc_train)\n",
        "    # print(\"\\nAcc Train: \"+str(acc_train)+\", Loss Train:\"+str(loss_train)+\"\\n\")\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    # print(\"\\nNow in Validation: \\n\")\n",
        "    output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = metrics.accuracy_score(pred_labels[idx_val].cpu().detach().numpy(),\n",
        "                                     labels[idx_val].cpu().detach().numpy())\n",
        "    \n",
        "    performance_file = open(file_name+\"_performance\", \"a+\")\n",
        "    print(loss_val,acc_val)\n",
        "\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "  \n",
        "    performance_file.write('Epoch: {:04d}'.format(epoch+1)+';'+\n",
        "          'loss_train: {:.4f}'.format(loss_train.item())+';'+\n",
        "          'acc_train: {:.4f}'.format(acc_train.item())+';'+\n",
        "          'loss_val: {:.4f}'.format(loss_val.item())+';'+\n",
        "          'acc_val: {:.4f}'.format(acc_val.item())+';'+\n",
        "          'time: {:.4f}s'.format(time.time() - t)+'\\n')\n",
        "    performance_file.close()\n",
        "\n",
        "    return acc_val\n",
        "\n",
        "\n",
        "def test(model, features, adj, labels, idx_test):\n",
        "    model.eval()\n",
        "    # print(\"\\nNow in Testing: \\n\")\n",
        "    output = model(features, adj)\n",
        "    pred_labels = torch.argmax(output, axis=1)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = metrics.accuracy_score(labels[idx_test].cpu().detach().numpy(),\n",
        "                                      pred_labels[idx_test].cpu().detach().numpy())\n",
        "    f1_test = metrics.f1_score(labels[idx_test].cpu().detach().numpy(), pred_labels[idx_test].cpu().detach().numpy(),\n",
        "                               average='weighted')\n",
        "    auc_test = metrics.roc_auc_score(one_hot(labels[idx_test].cpu().detach().numpy()),\n",
        "                                     output[idx_test].cpu().detach().numpy(), multi_class='ovr', average='weighted')\n",
        "    \n",
        "    # print('loss_test: {:.4f}'.format(loss_test.item()),\n",
        "    #       'acc_test: {:.4f}'.format(acc_test.item()),\n",
        "    #       'auc_test: {:.4f}'.format(auc_test.item()),\n",
        "    #       'f1_test: {:.4f}'.format(f1_test.item()),'\\n')\n",
        "\n",
        "    return loss_test.item(), acc_test, auc_test, f1_test\n",
        "\n",
        "\n",
        "def single_train_and_test(lambda_matrix, Probability_matrix, features, adj, labels, idx_train, idx_val, idx_test,\n",
        "                          model_type, normalize=False, file_name='pop'):\n",
        "    # print(\"\\nFeature Shape: \",features.shape)\n",
        "    # print(\"\\nadj Shape: \",adj.shape)\n",
        "\n",
        "\n",
        "    if model_type==\"DynAERNN\":\n",
        "        \n",
        "        length=adj.shape[1]\n",
        "        lookup=length-2\n",
        "\n",
        "        dim_emb  = class_num\n",
        "        if args_cuda:\n",
        "          tensorflow.device('/gpu:0')\n",
        "        embedding = DynAERNN(d   = dim_emb,\n",
        "            beta           = 5,\n",
        "            n_prev_graphs  = lookup,\n",
        "            nu1            = 1e-6,\n",
        "            nu2            = 1e-6,\n",
        "            n_aeunits      = [50, 30],\n",
        "            n_lstmunits    = [50,dim_emb],\n",
        "            rho            = 0.3,\n",
        "            n_iter         = args_epochs,\n",
        "            xeta           = 1e-3,\n",
        "            n_batch        = 10,\n",
        "            modelfile      = ['./intermediate/enc_model_dynAERNN.json', \n",
        "                              './intermediate/dec_model_dynAERNN.json'],\n",
        "            weightfile     = ['./intermediate/enc_weights_dynAERNN.hdf5', \n",
        "                              './intermediate/dec_weights_dynAERNN.hdf5'],\n",
        "            savefilesuffix = \"testing\")\n",
        "        embs = []\n",
        "        \n",
        "        graphs     = [nx.Graph(adj[:,l,:].numpy()) for l in range(length)]\n",
        "        for temp_var in range(lookup, length):\n",
        "                        emb, _ = embedding.learn_embeddings(graphs[:temp_var])\n",
        "                        embs.append(emb)\n",
        "        centroid=kmeans(embs[-1],class_num)[0] #change kSigvec from complex64 to float\n",
        "        result=vq(embs[-1],centroid)[0]\n",
        "\n",
        "        \n",
        "\n",
        "        perm = permutations(range(class_num)) \n",
        "        one_hot_result=torch.tensor(one_hot(result,class_num))\n",
        "        acc_test=0\n",
        "        f1_test=0\n",
        "        auc_test=0\n",
        "        count=0\n",
        "        for i in perm: \n",
        "              count+=1\n",
        "              one_hot_i=one_hot(np.array(i))\n",
        "              perm_result=torch.mm(one_hot_result,torch.tensor(one_hot_i))\n",
        "              pred_labels=torch.argmax(perm_result,axis=1)\n",
        "              acc_test = max(metrics.accuracy_score(labels,pred_labels),acc_test)\n",
        "              f1_test=max(metrics.f1_score(labels, pred_labels,average='weighted'),f1_test)\n",
        "              auc_test=max(metrics.roc_auc_score(one_hot(labels), perm_result,multi_class='ovr',average='weighted'),auc_test)\n",
        "              if count%10000==0:\n",
        "                print(count)\n",
        "                print(acc_test,f1_test,auc_test)   \n",
        "        print(str(acc_test)+'\\t'+str(f1_test)+'\\t'+str(auc_test))  \n",
        "        try:\n",
        "              spec_norm=getKlargestSigVec(adj-Probability_matrix,2)[0]\n",
        "        except:\n",
        "              spec_norm=[]\n",
        "        # return 0,acc_test,spec_norm\n",
        "        return loss, acc_test, auc_test, f1_test, spec_norm\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # choose adj matrix\n",
        "    # GCN:n*n, Others: n*t*n\n",
        "    elif model_type == 'GCN':\n",
        "        if type(lambda_matrix) != type(None):\n",
        "            decay_adj = torch.zeros(adj.shape[0], adj.shape[0])\n",
        "            for j in range(adj.shape[0]):\n",
        "                for k in range(adj.shape[2]):\n",
        "                    decay_adj[j][k] = lambda_matrix[labels[j]][labels[k]]\n",
        "            now_adj = adj[:, 0, :].clone()\n",
        "\n",
        "            for i in range(1, adj.shape[1]):  # time_steps\n",
        "                tmp_adj = adj[:, i, :].clone()\n",
        "\n",
        "                now_adj = (1 - decay_adj) * now_adj + decay_adj * tmp_adj\n",
        "            adj = now_adj\n",
        "        else:\n",
        "            now_adj = adj[:, 0, :].clone()\n",
        "            for i in range(1, adj.shape[1]):  # time_steps\n",
        "                now_adj += adj[:, i, :].clone()\n",
        "            adj = now_adj\n",
        "\n",
        "        # normalize in both cases\n",
        "        if normalize == True:\n",
        "            adj += torch.eye(adj.shape[0], adj.shape[1])\n",
        "            d = torch.sum(adj, axis=1)\n",
        "            D_minus_one_over_2 = torch.zeros(adj.shape[0], adj.shape[0])\n",
        "            D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d ** (-0.5)\n",
        "            adj = torch.mm(torch.mm(D_minus_one_over_2, adj), D_minus_one_over_2)\n",
        "\n",
        "        features = features[:, -1, :]\n",
        "\n",
        "\n",
        "    elif model_type == 'GAT' or model_type == 'GraphSage':\n",
        "        now_adj = adj[:, 0, :].clone()\n",
        "        for i in range(1, adj.shape[1]):  # time_steps\n",
        "            now_adj += adj[:, i, :].clone()\n",
        "        adj = now_adj\n",
        "\n",
        "        # normalize in both cases\n",
        "        if normalize == True:\n",
        "            adj += torch.eye(adj.shape[0], adj.shape[1])\n",
        "            d = torch.sum(adj, axis=1)\n",
        "            D_minus_one_over_2 = torch.zeros(adj.shape[0], adj.shape[0])\n",
        "            D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d ** (-0.5)\n",
        "            adj = torch.mm(torch.mm(D_minus_one_over_2, adj), D_minus_one_over_2)\n",
        "\n",
        "        features = features[:, -1, :]\n",
        "    elif model_type == 'EGCN':\n",
        "        adj = torch.transpose(adj, 0, 1)\n",
        "        features = torch.transpose(features, 0, 1)\n",
        "\n",
        "\n",
        "    elif model_type == 'GAT_BiLSTM':\n",
        "        now_adj = adj[:, 0, :].clone()\n",
        "        for i in range(1, adj.shape[1]):  # time_steps\n",
        "            now_adj += adj[:, i, :].clone()\n",
        "        adj = now_adj\n",
        "\n",
        "        if normalize == True:\n",
        "            adj += torch.eye(adj.shape[0], adj.shape[1])\n",
        "            d = torch.sum(adj, axis=1)\n",
        "            D_minus_one_over_2 = torch.zeros(adj.shape[0], adj.shape[0])\n",
        "            D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d ** (-0.5)\n",
        "            adj = torch.mm(torch.mm(D_minus_one_over_2, adj), D_minus_one_over_2)\n",
        "\n",
        "        features = features[:, -1, :]\n",
        "\n",
        "    elif model_type == 'GraphSage_BiLSTM_GAT':\n",
        "        now_adj = adj[:, 0, :].clone()\n",
        "        for i in range(0, adj.shape[1]):  # time_steps\n",
        "          now_adj = adj[:, 0, :].clone()\n",
        "        adj = now_adj\n",
        "        \n",
        "\n",
        "        if normalize == True:\n",
        "            adj += torch.eye(adj.shape[0], adj.shape[1])\n",
        "            d = torch.sum(adj, axis=1)\n",
        "            D_minus_one_over_2 = torch.zeros(adj.shape[0], adj.shape[0])\n",
        "            D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d ** (-0.5)\n",
        "            adj = torch.mm(torch.mm(D_minus_one_over_2, adj), D_minus_one_over_2)\n",
        "\n",
        "        features = features[:, -1, :]\n",
        "    elif model_type == \"GraphSageTransformer\":\n",
        "        now_adj = adj[:, 0, :].clone()\n",
        "        for i in range(1, adj.shape[1]):  # time_steps\n",
        "            now_adj += adj[:, i, :].clone()\n",
        "        adj = now_adj\n",
        "\n",
        "        # normalize in both cases\n",
        "        if normalize == True:\n",
        "            adj += torch.eye(adj.shape[0], adj.shape[1])\n",
        "            d = torch.sum(adj, axis=1)\n",
        "            D_minus_one_over_2 = torch.zeros(adj.shape[0], adj.shape[0])\n",
        "            D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d ** (-0.5)\n",
        "            adj = torch.mm(torch.mm(D_minus_one_over_2, adj), D_minus_one_over_2)\n",
        "\n",
        "        features = features[:, -1, :]\n",
        "    elif model_type=='EGCN':\n",
        "        adj=torch.transpose(adj,0,1)\n",
        "        features=torch.transpose(features,0,1)\n",
        "\n",
        "\n",
        "    # define model\n",
        "    if model_type == 'GCN':\n",
        "        model = GCN(nfeat=features.shape[1],\n",
        "                    nhid=args_hidden,\n",
        "                    nclass=class_num,\n",
        "                    dropout=args_dropout)\n",
        "    elif model_type == 'RNNGCN':\n",
        "        model = RNNGCN(nfeat=features.shape[2],\n",
        "                       nhid=args_hidden,\n",
        "                       nclass=class_num,\n",
        "                       dropout=args_dropout)\n",
        "    elif model_type == 'TRNNGCN':\n",
        "        model = TRNNGCN(nfeat=features.shape[2],\n",
        "                        nhid=args_hidden,\n",
        "                        nclass=class_num,\n",
        "                        dropout=args_dropout,\n",
        "                        nnode=features.shape[0],\n",
        "                        use_cuda=args_cuda)\n",
        "    elif model_type=='GCNLSTM':\n",
        "        model = GCNLSTM(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout)\n",
        "        \n",
        "    elif model_type == 'MyGNN':\n",
        "        # print(\"\\nIn MyGNN Feature Shape: \",features.shape)\n",
        "        # print(\"\\nadj Shape: \",adj.shape)\n",
        "\n",
        "        model = MyGNN(nfeat=features.shape[2],\n",
        "                        nhid=args_hidden,\n",
        "                        nclass=class_num,\n",
        "                        dropout=args_dropout)\n",
        "\n",
        "    elif model_type == \"GAT\":\n",
        "        # print(\"\\nIn GAT Feature Shape: \",features.shape)\n",
        "        # print(\"\\nadj Shape: \",adj.shape)\n",
        "        adj = dgl.from_networkx(nx.Graph(adj.numpy()))  # fit in dgl\n",
        "        # print(\"\\nAfter DGL Conv adj Shape: \",adj.shape)\n",
        "        model = GAT(nfeat=features.shape[1],\n",
        "                    nhid=args_hidden,\n",
        "                    nclass=class_num,\n",
        "                    dropout=args_dropout)\n",
        "    elif model_type == \"GraphSage\":\n",
        "        adj = dgl.from_networkx(nx.Graph(adj.numpy()))  # fit in dgl\n",
        "        model = GraphSage(nfeat=features.shape[1],\n",
        "                          nhid=args_hidden,\n",
        "                          nclass=class_num,\n",
        "                          dropout=args_dropout)\n",
        "\n",
        "    elif model_type == \"GAT_BiLSTM\":\n",
        "        adj = dgl.from_networkx(nx.Graph(adj.numpy()))  # fit in dgl\n",
        "        model = GAT(nfeat=features.shape[1],\n",
        "                    nhid=args_hidden,\n",
        "                    nclass=class_num,\n",
        "                    dropout=args_dropout)\n",
        "    elif model_type == \"GraphSage_BiLSTM_GAT\":\n",
        "        adj = dgl.from_networkx(nx.Graph(adj.numpy()))  # fit in dgl\n",
        "        model = GraphSage_BiLSTM_GAT(nfeat=features.shape[1],\n",
        "                                     nhid=args_hidden,\n",
        "                                     nclass=class_num,\n",
        "                                     dropout=args_dropout)\n",
        "    elif model_type == \"GraphSageTransformer\":\n",
        "        adj = dgl.from_networkx(nx.Graph(adj.numpy()))  # fit in dgl\n",
        "        model = GraphSageTransformer(nfeat=features.shape[1],\n",
        "                                     nhid=args_hidden,\n",
        "                                     nclass=class_num,\n",
        "                                     dropout=args_dropout)\n",
        "        \n",
        "    elif model_type == 'GGGGG':\n",
        "        model = GGGGG(nfeat=features.shape[2],\n",
        "                        nhid=args_hidden,\n",
        "                        nclass=class_num,\n",
        "                        dropout=args_dropout)\n",
        "    elif model_type==\"EGCN\":\n",
        "        model = EGCN(nfeat=features.shape[2],\n",
        "                    nhid=args_hidden,\n",
        "                    nclass=class_num,\n",
        "                    device=torch.device('cpu'))\n",
        "\n",
        "    if model_type != \"SPEC\" and model_type != \"SPEC_sklearn\" and model_type != \"DynAERNN\":\n",
        "        if args_cuda:\n",
        "            if model_type != 'EGCN':\n",
        "                model = model.to(torch.device('cuda:0'))  # .cuda()\n",
        "                features = features.cuda()\n",
        "                adj = adj.to(torch.device('cuda:0'))\n",
        "                labels = labels.cuda()\n",
        "                idx_train = idx_train.cuda()\n",
        "                idx_val = idx_val.cuda()\n",
        "                idx_test = idx_test.cuda()\n",
        "        # optimizer and train\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=args_lr, weight_decay=args_weight_decay)\n",
        "        # Train model\n",
        "        train_time_1 = time.time()\n",
        "        best_val = 0\n",
        "        for epoch in range(args_epochs):\n",
        "            acc_val = train(epoch, model, optimizer, features, adj, labels, idx_train, idx_val, model_type,file_name)\n",
        "            # print(\"\\nTrain Acc= \" + str(acc_val) + \"\\n\")\n",
        "            # print(model.Lambda)\n",
        "            if acc_val > best_val:\n",
        "                best_val = acc_val\n",
        "                loss, acc, auc, f1 = test(model, features, adj, labels, idx_test)\n",
        "                test_best_val = [loss, acc, auc, f1]\n",
        "        train_time_2 = time.time()\n",
        "        # print(\"\\n^^^^^^TIME^^^^^^^\\n\")\n",
        "        print(train_time_2-train_time_1)\n",
        "        # Testing\n",
        "        # loss, acc, auc, f1 = test(model, features, adj, labels, idx_test)\n",
        "        if model_type == 'RNNGCN' or model_type == 'TRNNGCN':\n",
        "            print(model.Lambda, end='\\t')\n",
        "        # print(\"\\n\"+str(test_best_val)+\"\\n\")\n",
        "        print(\"acc= \" + str(acc) + \", auc= \" + str(auc) + \", f1= \" + str(f1) + \"\\n\")\n",
        "        full_list.append((acc,auc,f1))\n",
        "        # print(str(test_best_val[1]) + '\\t' + str(test_best_val[2]) + '\\t' + str(test_best_val[3]))  # ,end='\\t')\n",
        "        try:\n",
        "            spec_norm = getKlargestSigVec(now_adj - Probability_matrix, 2)[0]\n",
        "        except:\n",
        "            spec_norm = 0  # temperal adj\n",
        "        return loss, acc, auc, f1, spec_norm\n",
        "\n",
        "\n",
        "\"\"\"# Run Exp for Spectral Clustering and GCN with Decay Rates\n",
        "\n",
        "# Run Exp on Simulated and Real Datasets\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def load_real_data(dataset_name):\n",
        "    print(dataset_name)\n",
        "    dataset_dict = dict()\n",
        "    # dataset_dict[\"DBLP3\"] = \"DBLP3.npz\"\n",
        "    # dataset_dict[\"DBLP5\"] = \"DBLP5.npz\"\n",
        "    # dataset_dict[\"Brain\"] = \"Brain.npz\"\n",
        "    # dataset_dict[\"Reddit\"] = \"reddit.npz\"\n",
        "    # dataset_dict[\"DBLPE\"] = \"DBLPE.npz\"\n",
        "    dataset_dict[\"DBLP3\"]=\"/content/drive/MyDrive/InterpretableClustering-master/DBLP3.npz\"\n",
        "    dataset_dict[\"DBLP5\"]=\"/content/drive/MyDrive/InterpretableClustering-master/DBLP5.npz\"\n",
        "    dataset_dict[\"Brain\"]=\"/content/drive/MyDrive/InterpretableClustering-master/Brain.npz\"\n",
        "    # dataset_dict[\"Reddit\"]=\"/content/drive/MyDrive/InterpretableClustering-master/reddit.npz\"\n",
        "    dataset_dict[\"Reddit\"]=\"/content/drive/MyDrive/GCN-SE-main/reddit.npz\"\n",
        "    dataset_dict[\"DBLPE\"]=\"/content/drive/MyDrive/InterpretableClustering-master/DBLPE.npz\"\n",
        "\n",
        "    print(dataset_dict[dataset_name])\n",
        "\n",
        "    dataset = np.load(dataset_dict[dataset_name])\n",
        "    # print(dict(dataset))\n",
        "    # print(dataset['adjs'].shape)\n",
        "    # print(dataset['labels'].shape)\n",
        "\n",
        "    Graphs = torch.LongTensor(dataset['adjs'])  # (n_time, n_node, n_node)\n",
        "    Graphs = torch.transpose(Graphs, 0, 1)  # (n_node, n_time, n_node)\n",
        "\n",
        "    now_adj = Graphs[:, 0, :].clone()\n",
        "    # print(torch.sum(now_adj))\n",
        "    # print(now_adj,now_adj.shape)\n",
        "    for i in range(1, Graphs.shape[1]):  # time_steps\n",
        "        # print(\"\\nTime:\",str(i),\"\\n\")\n",
        "        # print(now_adj,now_adj.shape)\n",
        "        now_adj += Graphs[:,i,:].clone()\n",
        "\n",
        "    #     # print(now_adj,now_adj.shape)\n",
        "    #     # print(torch.sum(now_adj))\n",
        "\n",
        "    d = torch.sum(now_adj, axis=1)\n",
        "    # print(d,d.shape)\n",
        "    non_zero_index = torch.nonzero(d, as_tuple=True)[0]\n",
        "    # print(non_zero_index,non_zero_index.shape)\n",
        "    Graphs = Graphs[non_zero_index, :, :]\n",
        "    Graphs = Graphs[:, :, non_zero_index]\n",
        "    # print(Graphs.shape)\n",
        "\n",
        "\n",
        "    if dataset_name == \"DBLPE\":\n",
        "        Labels = torch.LongTensor(np.argmax(dataset['labels'], axis=2))  # (n_node, n_time, num_classes) argmax\n",
        "        Features = torch.zeros(Graphs.shape)\n",
        "        for i in range(Features.shape[1]):\n",
        "            Features[:, i, :] = torch.eye(Features.shape[0], Features.shape[2])\n",
        "        Labels = Labels[non_zero_index]\n",
        "\n",
        "    else:\n",
        "        Labels = torch.LongTensor(np.argmax(dataset['labels'], axis=1))  # (n_node, num_classes) argmax\n",
        "        Features = torch.LongTensor(dataset['attmats'])  # (n_node, n_time, att_dim)\n",
        "\n",
        "        Features = Features[non_zero_index]\n",
        "        Labels = Labels[non_zero_index]\n",
        "    # print(Graphs,Graphs.shape)\n",
        "    # print(Features.float().shape, Graphs.float().shape, Labels.long().shape,sep=\"\\n-----------\\n\")\n",
        "\n",
        "    # shuffle datasets\n",
        "    number_of_nodes = Graphs.shape[0]\n",
        "    # print(\"\\nNumber of Total Nodes:\\n\")\n",
        "    # print(number_of_nodes)\n",
        "    nodes_id = list(range(number_of_nodes))\n",
        "    # print(nodes_id)\n",
        "    random.shuffle(nodes_id)\n",
        "    # print(len(nodes_id))\n",
        "    # idx_train = torch.LongTensor(nodes_id[:(5 * number_of_nodes) // 10])\n",
        "    # idx_val = torch.LongTensor(nodes_id[(5 * number_of_nodes) // 10: (7 * number_of_nodes) // 10])\n",
        "    # idx_test = torch.LongTensor(nodes_id[(7 * number_of_nodes) // 10: number_of_nodes])\n",
        "    \n",
        "    idx_train = torch.LongTensor(nodes_id[:(7*number_of_nodes)//10])\n",
        "    idx_val = torch.LongTensor(nodes_id[(7*number_of_nodes)//10: (9*number_of_nodes)//10])\n",
        "    idx_test = torch.LongTensor(nodes_id[(9*number_of_nodes)//10: number_of_nodes])    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    # print(\"\\nOriginal Features:\\n\")\n",
        "    # print(Features.float(),Features.float().shape)\n",
        "    # Features=torch.rand(Features.float().shape)\n",
        "    # print(\"\\Random Features:\\n\")\n",
        "    # print(Features.float(),Features.float().shape)\n",
        "    # print(Features.float(), Graphs.float(), Labels.long(), idx_train, idx_val, idx_test,sep=\"\\n-----------\\n\")\n",
        "    # print(features.shape,adj.shape,labels.shape, idx_train.shape, idx_val.shape, idx_test.shape)\n",
        "    return Features.float(), Graphs.float(), Labels.long(), idx_train, idx_val, idx_test, []\n",
        "\n",
        "\n",
        "\"\"\"/content/drive/MyDrive/Dynamic_New\"\"\"\n",
        "\n",
        "\n",
        "def test_real_dataset(file_name):\n",
        "    if IN_COLAB == True:\n",
        "        # summary_file = open(\"/content/drive/MyDrive/Finetune/\"+file_name, \"a+\")\n",
        "        summary_file = open(file_name, \"a+\")\n",
        "    else:\n",
        "        summary_file = open(file_name, \"a+\")\n",
        "        # summary_file = open(\"/content/drive/MyDrive/Finetune\"+file_name, \"a+\")\n",
        "    t = time.time()\n",
        "    lambda_matrix = None\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    total_norm = []\n",
        "    loss, acc, auc, f1, specnorm = single_train_and_test(lambda_matrix, Probability_matrix, features, adj, labels,\n",
        "                                                         idx_train,\n",
        "                                                         idx_val, idx_test, model_type, normalize=args_normalize,file_name=file_name)\n",
        "    if type(lambda_matrix) != type(None):\n",
        "        summary_file.write(\"accuracy= {:.6f}\".format(acc) +\n",
        "                           \"\\tauc= {:.6f}\".format(auc) +\n",
        "                           \"\\tf1= {:.6f}\".format(f1) +\n",
        "                           \"\\n\")\n",
        "    else:\n",
        "        summary_file.write(\"accuracy= {:.6f}\".format(acc) +\n",
        "                           \"\\tauc= {:.6f}\".format(auc) +\n",
        "                           \"\\tf1= {:.6f}\".format(f1) +\n",
        "                           \"\\n\")\n",
        "\n",
        "    summary_file.close()\n",
        "    return file_name\n",
        "\n",
        "\n",
        "\n",
        "total_time_3 = time.time()\n",
        "\n",
        "\n",
        "full_list=[]\n",
        "# dataset_name=\"DBLPE\"\n",
        "# dataset_name = \"DBLP5\"\n",
        "# dataset_name = \"DBLP3\"\n",
        "# dataset_name=\"Reddit\"\n",
        "dataset_name=\"Brain\"\n",
        "features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = load_real_data(dataset_name)\n",
        "print(features.shape,adj.shape,labels.shape, idx_train.shape, idx_val.shape, idx_test.shape)\n",
        "class_num = int(labels.max()) + 1\n",
        "print(class_num)\n",
        "total_adj = adj\n",
        "total_labels = labels\n",
        "\n",
        "# print(features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix)\n",
        "\n",
        "# len(total_labels)\n",
        "model_type = 'MyGNN'  # GCN, GAT, GraphSage, GCNLSTM, EGCN, RNNGCN, TRNNGCN, MyGNN\n",
        "args_hidden = class_num\n",
        "args_dropout = 0.5\n",
        "args_lr = 0.0025\n",
        "args_weight_decay = 5e-4\n",
        "args_epochs = 30\n",
        "args_no_cuda = True\n",
        "args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
        "args_normalize = True\n",
        "file_name = dataset_name + '_' + model_type + \".txt\"\n",
        "\n",
        "# file_name = \"/content/drive/MyDrive/SP_CIKM_all_30/\"+dataset_name + '_' + model_type + \".txt\"\n",
        "print(\"\\n\"+model_type+\"\\n\")\n",
        "for i in range(30):\n",
        "    print(\"\\nIteration: \"+str(i)+\"\\n\")\n",
        "    test_real_dataset(file_name)\n",
        "print(full_list)\n",
        "\n",
        "def Average(lst):\n",
        "    return sum(lst) / len(lst)\n",
        "acc_list= [x for (x,y,z) in full_list]\n",
        "auc_list= [y for (x,y,z) in full_list]\n",
        "f1_list= [z for (x,y,z) in full_list]\n",
        "print(len(acc_list))\n",
        "print(Average(acc_list),Average(auc_list),Average(f1_list),sep=',')\n",
        "total_time_4 = time.time()\n",
        "print(total_time_4-total_time_3)\n",
        "# my_list = []\n",
        "# with open(file_name) as fp:\n",
        "#     Lines = fp.readlines()\n",
        "#     for line in Lines:\n",
        "#         a = line.strip()\n",
        "#         my_list.append([float(a.split('\\t')[i].split('= ')[1]) for i in [0, 1, 2]])\n",
        "\n",
        "\n",
        "# df = pd.DataFrame(my_list, columns=['acc', 'auc', 'f1'])\n",
        "# my_dict=dict(df.mean())\n",
        "# summary_file = open(file_name, \"a+\")\n",
        "# summary_file.write(\"break\\n\"+str(my_dict['acc'])+\", \"+str(my_dict['auc'])+\", \"+str(my_dict['f1'])+\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_-2LQgBUcO7"
      },
      "source": [
        "# New section"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Temporal_Graph.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
